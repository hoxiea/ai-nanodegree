
# Kelleher 2015, Chapter 6: Probability-Based Learning (Naive Bayes)

There were a couple of nice "a-ha!" moments for me while reading this chapter.

I was familiar with a lot of the probability theory, and with the important idea of updating your beliefs about the world in response to evidence you received. What **wasn't** as clear to me as I would have liked was the simplifying assumption that Naive Bayes. I knew it had something to do with conditional independence, but I now have a much better idea of what's going on, thanks to the crystal clear breakdown on Page 263.

I also **loved** the context that they established for graphical models, too. I audited Mike Jordan's "Probabilistic Graphical Models" while I was a Statistics PhD student at Berkeley, and there were moments in that class where I felt like I came closer to glimpsing the code underlying The Matrix than I ever had. But there was a lot of intricate detail, too, and it was hard to establish a good "big picture" mental model for what I was doing and why I was doing it.

Kelleher et. al's positioning of Bayesian networks between the full total-probability model and the completely decoupled Naive Bayes model makes a ton of sense, and gives me a much richer appreciation of what I know about graphical models.
