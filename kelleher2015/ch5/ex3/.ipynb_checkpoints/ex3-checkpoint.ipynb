{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kelleher 2015, Chapter 5, Exercise 3\n",
    "\n",
    "In this exercise, we're predicting the level of corruption (**continuous variable**) in a country based on macroeconomic and social features.\n",
    "\n",
    "The data are available here: http://bit.ly/kelleher2015-ch5-ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Read in the training data AND the new data\n",
    "input_file = \"ch5ex3.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Extract and process the new data (Russia)\n",
    "new_data = df.tail(1)\n",
    "new_data = new_data.drop(\"CPI\", 1)\n",
    "new_data = new_data.rename(new_data.Country)\n",
    "new_data = new_data.drop(\"Country\", 1)\n",
    "df = df.drop(df.tail(1).index)\n",
    "\n",
    "# Get the training data ready to go\n",
    "target_colname = \"CPI\"\n",
    "X = df.drop(target_colname, axis=1)\n",
    "y = df[target_colname]\n",
    "\n",
    "X = X.rename(X.Country)\n",
    "X = X.drop(\"Country\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) k=3, Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.58913333])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 3\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Fit the model\n",
    "clf_3a = neighbors.KNeighborsRegressor(n_neighbors=k, metric=metric)\n",
    "clf_3a.fit(X, y)\n",
    "\n",
    "# What's the predicted CPI for Russia?\n",
    "clf_3a.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we predict a CPI of approximately 4.5891 for Russia using the average CPIs of the $k=3$ nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) k = 16, Euclidean distance, $w_i = \\frac{1}{d_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.09378991])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 16\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Fit the model\n",
    "clf_3b = neighbors.KNeighborsRegressor(n_neighbors=k, metric=metric, weights=\"distance\")\n",
    "clf_3b.fit(X, y)\n",
    "clf_3b.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted kNN prediction moder predicts a CPI of approximately 6.0937 for Russia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c) k = 3, Euclidean distance, Normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in the section, when you're doing distance-based work, the scale of the various variables is extremely important. A variable with a naturally larger scale can dominate a Euclidean distance calculation, for example - the example on page 205 does (in my opinion) a great job of illustrating this.\n",
    "\n",
    "To get all of our variables on the same scale, we can normalize them using **range normalization**. After deciding the range of values we want each variable to span (here we'll use low=0, high=1), we normalize as follows:\n",
    "\n",
    "$$a_i' = low + \\frac{a_i - \\min(a)}{\\max(a) - \\min(a)} \\times (high - low)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = pd.DataFrame(mm_scaler.fit_transform(X))\n",
    "X_scaled.columns = list(X)\n",
    "\n",
    "new_data_scaled = mm_scaler.transform(new_data)\n",
    "\n",
    "# Configure the algorithm\n",
    "k = 5\n",
    "metric = \"euclidean\"\n",
    "weights=\"distance\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2c = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric, weights=weights)\n",
    "clf_2c.fit(X, y)\n",
    "clf_2c.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing to note here is that **$k$ is equal to the total number of training observations we have**, so we're letting all the data we have vote on the classification, where their votes are weighted based on how far away from the new observation they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d) k = 3, Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 3\n",
    "metric = \"manhattan\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2d = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "clf_2d.fit(X, y)\n",
    "clf_2d.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e) k=3, Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.53033009],\n",
       "       [ 0.28867513],\n",
       "       [ 0.4330127 ],\n",
       "       [ 0.8660254 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(X, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So observations 2, 4, and 5 are most similar in terms of cosines. The majority of these have label \"Ham,\" so that's the prediction we make with $k=3$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
