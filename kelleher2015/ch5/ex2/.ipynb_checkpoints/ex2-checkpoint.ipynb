{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kelleher 2015, Chapter 5, Exercise 2\n",
    "\n",
    "In this exercise, we're going to use a bag-of-words model with KNN to classify emails as spam/ham.\n",
    "\n",
    "The data are available here: http://bit.ly/kelleher2015-ch5-ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Read in the training data\n",
    "input_file = \"ex2data.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "target_colname = \"Spam\"\n",
    "X = df.drop(target_colname, axis=1)\n",
    "y = df[target_colname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) k=1, Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 1\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2a = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "clf_2a.fit(X, y)\n",
    "\n",
    "# Create the data we'd like to make predictions for\n",
    "def email_to_keyword_counts(X, email):\n",
    "    keywords = [s.lower() for s in list(X)]\n",
    "    email_words = email.lower().split(\" \")\n",
    "    new_row = [(s.title(), [1 if s in email_words else 0]) for s in keywords]\n",
    "    return new_row\n",
    "\n",
    "email = \"machine learning for free\"\n",
    "new_data = pd.DataFrame.from_items(email_to_keyword_counts(X, email))\n",
    "new_data.columns = list(X)\n",
    "\n",
    "# What's the target level of the point nearest to our input email?\n",
    "clf_2a.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like very reasonable predictions:\n",
    "\n",
    "* The first new data point is very similar to the fourth observation in the training data (index = 3).\n",
    "* The second new data point is very similar to the last observation in the training data (index = 5).\n",
    "* The third new data point is very similar to the third observation in the training data (index = 2).\n",
    "\n",
    "And since we're only using k=1, the target values for these closest points are the predictions we'll make for our new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction of \"False\" makes sense for this email - \"Machine\" and \"Learning\" appeared only in ham messages, and this new email is extremely close to our last training data example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) k = 3, Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True], dtype=bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 3\n",
    "metric = \"euclidean\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2b = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "clf_2b.fit(X, y)\n",
    "clf_2b.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the next-closest two observations were both spam, so now the majority vote concludes \"Spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c) k = 5, Euclidean distance, $w_i = \\frac{1}{d_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 5\n",
    "metric = \"euclidean\"\n",
    "weights=\"distance\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2c = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric, weights=weights)\n",
    "clf_2c.fit(X, y)\n",
    "clf_2c.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing to note here is that **$k$ is equal to the total number of training observations we have**, so we're letting all the data we have vote on the classification, where their votes are weighted based on how far away from the new observation they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d) k = 3, Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False], dtype=bool)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the algorithm\n",
    "k = 3\n",
    "metric = \"manhattan\"\n",
    "\n",
    "# Fit the model\n",
    "clf_2d = neighbors.KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "clf_2d.fit(X, y)\n",
    "clf_2d.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e) k=3, Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.53033009],\n",
       "       [ 0.28867513],\n",
       "       [ 0.4330127 ],\n",
       "       [ 0.8660254 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(X, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So observations 2, 4, and 5 are most similar in terms of cosines. The majority of these have label \"Ham,\" so that's the prediction we make with $k=3$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
